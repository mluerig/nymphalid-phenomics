{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "Converted from `01_feature_extraction.py`.\n",
    "\n",
    "This notebook runs the feature extraction pipeline: loading mask files, computing embeddings, extracting image features, and computing CLIP-based text similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "import phenopype as pp\n",
    "import phenopype_plugins as pp_plugins\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "# local utils\n",
    "from utils import utils_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set working directory\n",
    "os.chdir(r\"D:/git-repos/mluerig/nymphalid-phenomics/\")\n",
    "\n",
    "# optional: turn off phenopype verbosity\n",
    "pp.config.verbose = False\n",
    "\n",
    "# optional: suppress warnings (there will be a lot of them)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## check for cuda \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = f\"data_raw/segmentation_masks_clean/all_masks\"\n",
    "\n",
    "file_dict = {}\n",
    "for species_name in os.listdir(input_dir):\n",
    "    species_dir = os.path.join(input_dir, species_name)\n",
    "    for file_name in os.listdir(species_dir):\n",
    "        filepath_in = os.path.join(species_dir, file_name)\n",
    "        genus_name = species_name.split(\"_\")[0]\n",
    "        file_dict[file_name] = {\n",
    "            \"genus_name\" : genus_name,\n",
    "            \"species_name\" : species_name,\n",
    "            \"mask_name\" : file_name,\n",
    "            \"mask_path\" : filepath_in\n",
    "            }\n",
    "\n",
    "data_imgs = pd.DataFrame.from_dict(file_dict, orient=\"index\")\n",
    "data_imgs = data_imgs.sort_values(by=['genus_name',\"species_name\",\"mask_name\"], ascending=True)\n",
    "data_imgs.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## file I/O\n",
    "path_data_results = \"data_raw/tables/embeddings.csv\"\n",
    "\n",
    "## set of processed images\n",
    "dict_results = {}\n",
    "\n",
    "#%% encoder setup\n",
    "model, preprocessing = utils_python.setup_unicom(\"ViT-L/14@336px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# pick an example row (change index as needed)\n",
    "sample_idx = 0\n",
    "sample_row = data_imgs.iloc[sample_idx]\n",
    "print(\"Embedding:\", sample_row['mask_name'])\n",
    "\n",
    "## open and convert to tensor\n",
    "img = Image.open(sample_row['mask_path']).convert('RGB')\n",
    "tens = preprocessing(img)        # CPU tensor\n",
    "tens = tens.unsqueeze(0).to(device)\n",
    "\n",
    "## encode\n",
    "with torch.no_grad():\n",
    "    if device == 'cuda':\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            out = model(tens)\n",
    "    else:\n",
    "        out = model(tens)\n",
    "\n",
    "## normalize and detach from GPU\n",
    "out = normalize(out)\n",
    "embedding_vector = out.cpu().squeeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "batch_size = 16 ## modify according to your GPU\n",
    "\n",
    "# Select rows that still need processing\n",
    "to_process = [row for _, row in data_imgs.iterrows() if row['mask_name'] not in dict_results]\n",
    "if not to_process:\n",
    "    print('Nothing to process (all images already embedded).')\n",
    "else:\n",
    "    total = len(to_process)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Running on device: {device}; batch_size={batch_size}\")\n",
    "    pbar = tqdm(total=total, desc='Embedding images', leave=True)\n",
    "\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch_rows = to_process[start:start + batch_size]\n",
    "        imgs = []\n",
    "        names = []\n",
    "        \n",
    "        # load + preprocess on CPU\n",
    "        for row in batch_rows:\n",
    "            try:\n",
    "                img = Image.open(row['mask_path']).convert('RGB')\n",
    "                tens = preprocessing(img)\n",
    "                imgs.append(tens)\n",
    "                names.append(row['mask_name'])\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped {row['mask_path']}: {e}\")\n",
    "        if not imgs:\n",
    "            pbar.update(len(batch_rows))\n",
    "            continue\n",
    "        \n",
    "        ## load into GPU\n",
    "        batch_tensor = torch.stack(imgs, dim=0).to(device)\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            if device == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out = model(batch_tensor)\n",
    "            else:\n",
    "                out = model(batch_tensor)\n",
    "            out = normalize(out)\n",
    "        out_np = out.cpu().detach().numpy()\n",
    "        for nm, vec in zip(names, out_np):\n",
    "            dict_results[nm] = vec\n",
    "\n",
    "        ## progress update\n",
    "        pbar.update(len(names))\n",
    "\n",
    "# final save\n",
    "data_results = pd.DataFrame.from_dict(dict_results, orient=\"index\")\n",
    "data_results.reset_index(inplace=True)\n",
    "data_results.rename(columns={'index': 'mask_name'}, inplace=True)\n",
    "data_results.to_csv(path_data_results, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## file I/O\n",
    "path_data_results = \"data_raw/tables/features.csv\"\n",
    "\n",
    "## set of processed images\n",
    "dict_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_intervall, skip_done = 10000, True\n",
    "pbar = tqdm(total=len(data_imgs), position=0, leave=False, desc=\"Extracting features...\")\n",
    "for idx1, row in data_imgs.iterrows():\n",
    "    if row[\"mask_name\"] in dict_results.keys() and skip_done:\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "    else:\n",
    "        image_BGRA = pp.load_image(row[\"mask_path\"])    \n",
    "        image, mask = image_BGRA[:,:,:3], image_BGRA[:,:,3]\n",
    "        annotations = pp.segmentation.detect_contour(mask, keep=\"largest\")\n",
    "        \n",
    "        ## blur image\n",
    "        image_blurred = pp.core.preprocessing.blur(image, 3)\n",
    "        image_blurred[mask == 0] = [0,0,0]\n",
    "        \n",
    "        # ## shape moments\n",
    "        # annotations = pp.measurement.compute_shape_moments(annotations, features=[\"basic\",\"hu_moments\"])\n",
    "        # shape = annotations[\"shape_features\"][\"a\"][\"data\"][\"shape_features\"][0]rename_dict = \n",
    "\n",
    "        \n",
    "        ## color moments\n",
    "        annotations = pp.measurement.compute_color_moments(image_blurred, annotations)\n",
    "        color_bgr = annotations[\"texture_features\"][\"a\"][\"data\"][\"texture_features\"][0]\n",
    "                \n",
    "        image_lab = pp.core.preprocessing.decompose_image(image_blurred, col_space=\"lab\")\n",
    "        annotations = pp.measurement.compute_color_moments(image_lab, annotations, channel_names=[\"light\", \"grre\", \"blyl\"])\n",
    "        color_lab = annotations[\"texture_features\"][\"a\"][\"data\"][\"texture_features\"][0]\n",
    "                \n",
    "        image_hls = pp.core.preprocessing.decompose_image(image_blurred, col_space=\"hls\")\n",
    "        annotations = pp.measurement.compute_color_moments(image_hls, annotations, channel_names=[\"hue\", \"lum\", \"sat\"])\n",
    "        color_hls = annotations[\"texture_features\"][\"a\"][\"data\"][\"texture_features\"][0]\n",
    "    \n",
    "        ## color bins\n",
    "        bins, clust = 5, 9\n",
    "        image_lab = pp.core.preprocessing.decompose_image(image_blurred, col_space=\"lab\")\n",
    "        recolor_res_hist_lab = pp_plugins.measurement.recolorize_binclust(\n",
    "            image_lab, mask, method=\"histogram\", bins_per_channel=bins, blur=1)\n",
    "        recolor_res_clust_lab = pp_plugins.measurement.recolorize_binclust(\n",
    "            image_lab, method=\"kmeans\", n_clusters=clust, blur=1)      \n",
    "        hue_bins = utils_python.channel_to_bins(\n",
    "            image_hls[:,:,0], mask=mask, blur=1, n_bins=36)\n",
    "\n",
    "        analyses = {\n",
    "            \"hist_lab\": recolor_res_hist_lab[\"pixel_assignments\"],\n",
    "            \"clust_lab\": recolor_res_clust_lab[\"pixel_assignments\"],\n",
    "            \"split_hue\": hue_bins\n",
    "        }\n",
    "        parsimony = {}\n",
    "        for key, data in analyses.items():\n",
    "            parsimony.update({f\"color_{key}_{k}\": v \n",
    "                              for k, v in utils_python.channel_parsimony(data, mask).items()})\n",
    "              \n",
    "        ## dfts \n",
    "        dft_stats = {}\n",
    "        for chan in [\"hue\", \"lum\", \"sat\"]:\n",
    "            dft_stats.update(pp.measurement.compute_DFT_stats(image_blurred, col_space=\"hls\", channel=chan))\n",
    "        for chan in [\"light\", \"grre\", \"blyl\"]:\n",
    "            dft_stats.update(pp.measurement.compute_DFT_stats(image_blurred, col_space=\"lab\", channel=chan))\n",
    "\n",
    "        ## concat\n",
    "        features = {**color_bgr, **color_hls, **color_lab, **parsimony, **dft_stats}\n",
    "        pbar.update(1)\n",
    "        dict_results[row[\"mask_name\"]] = features\n",
    "         \n",
    "# final save\n",
    "data_results = pd.DataFrame.from_dict(dict_results, orient=\"index\")\n",
    "data_results.reset_index(inplace=True)\n",
    "data_results.rename(columns={'index': 'mask_name'}, inplace=True)\n",
    "data_results.to_csv(path_data_results, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nymphalidae1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
